<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Yu (Andy) Huang </title> <meta name="author" content="Yu (Andy) Huang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://andypotatohy.github.io/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">Service </a> </li> <li class="nav-item "> <a class="nav-link" href="/books/">Bookshelf </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/example_pdf.pdf">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/outdoors/">CV_outdoors </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Yu (Andy)</span> Huang </h1> <p class="desc"><a href="#">Affiliations</a>. Address. Contacts. Motto. Etc.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic.jpg?v=7ca8212ae16123aa6984e497ae98c5b4" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>555 your office number</p> <p>123 your address street</p> <p>Your City, State 12345</p> </div> </div> <div class="clearfix"> <p>Write your biography here. Tell the world about yourself. Link to your favorite <a href="https://www.reddit.com" rel="external nofollow noopener" target="_blank">subreddit</a>. You can put a picture in, too. The code is already in, just name your picture <code class="language-plaintext highlighter-rouge">prof_pic.jpg</code> and put it in the <code class="language-plaintext highlighter-rouge">img/</code> folder.</p> <p>Put your address / P.O. box / other info right below your picture. You can also disable any of these elements by editing <code class="language-plaintext highlighter-rouge">profile</code> property of the YAML header of your <code class="language-plaintext highlighter-rouge">_pages/about.md</code>. Edit <code class="language-plaintext highlighter-rouge">_bibliography/papers.bib</code> and Jekyll will render your <a href="/al-folio/publications/">publications page</a> automatically.</p> <p>Link to your social media connections, too. This theme is set up to use <a href="https://fontawesome.com/" rel="external nofollow noopener" target="_blank">Font Awesome icons</a> and <a href="https://jpswalsh.github.io/academicons/" rel="external nofollow noopener" target="_blank">Academicons</a>, like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them.</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jan 15, 2016</th> <td> A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 07, 2015</th> <td> <a class="news-title" href="/news/announcement_2/">A long announcement with details</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 22, 2015</th> <td> A simple inline announcement. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Brahma2025.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Brahma2025.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="BRAHMA20251373" class="col-sm-8"> <div class="title">On the need of individually optimizing temporal interference stimulation of human brains due to inter-individual variability</div> <div class="author"> Tapasi Brahma, Alexander Guillen, Jeffrey Moreno, Abhishek Datta, and <em>Yu Huang</em> </div> <div class="periodical"> <em>Brain Stimulation</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S1935861X25002748" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=6dHFIA8AAAAJ&amp;citation_for_view=6dHFIA8AAAAJ:9Nmd_mFXekcC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-9-4285F4?logo=googlescholar&amp;labelColor=beige" alt="9 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Introduction Transcranial temporal interference stimulation (TI, TIS, or tTIS), also known as interferential stimulation (IFS), is able to focally stimulate deep brain regions, provided it is properly optimized. We previously presented an algorithm for optimizing TI using two arrays of electrodes and showed that it can achieve more focal stimulation compared to optimized high-definition transcranial electrical stimulation (HD-TES) and conventional optimized TI using two pairs of electrodes, especially in the deep brain areas such as the hippocampus. However, those modeling studies were only performed on an averaged head (MNI152 template) and three individual heads without exploring inter-individual variability. Existing TI works in the literature mostly utilize a common (possibly optimized) montage of two pairs of electrodes on different individual heads without considering inter-individual variability. Material and method: Here we aim to study the inter-individual variability of optimized TI by applying the same optimization algorithms on N = 25 heads using their individualized head models. Specifically, we compared the focality achieved by different stimulation techniques at six different regions of interest (ROI; right hippocampus, left dorsolateral prefrontal cortex, left motor cortex, right amygdala, right caudate, and left thalamus) under both individually optimized and unoptimized montages. We also conducted numerical sensitivity analysis on the individual optimization and performed phantom recordings to test our models. Results As expected, there is a variability in focality achieved by TI of up to 1.2 cm at the same ROI across subjects due to inter-individual differences in the head anatomy and tissue conductivity. We show that optimized TI using two arrays of electrodes achieves higher focality than that from optimized HD-TES at the same level of modulation intensity at 5 of the 6 ROIs. Compared to using a common montage either optimized from the MNI152 template or from the literature, individually optimized TI using two pairs of electrodes improves the focality by up to 4.4 cm, and by up to 1.1 cm if using two arrays of electrodes. Focality achieved by the individual optimization is sensitive to random changes and can vary up to 9.3 cm due to the non-lienarity of TI physics. Experimental recordings on a head phantom confirms the drop in TI stimulation strength when using unoptimized montages as predicted by our in silico models. Conclusion This work demonstrates the need of individually optimizing TI to target deep brain areas, and advocates against using a common head model and montage for TI modeling and experimental studies.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/hirschRadiologist2022.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="hirschRadiologist2022.jpeg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hirsch_radiologist-level_2022" class="col-sm-8"> <div class="title">Radiologist-Level Performance by Using Deep Learning for Segmentation of Breast Cancers on MRI Scans</div> <div class="author"> Lukas Hirsch<sup>*</sup>, <em>Yu Huang<sup>*</sup></em>, Shaojun Luo, Carolina Rossi Saccarelli, Roberto Lo Gullo, and <span class="more-authors" title="click to view 17 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '17 more authors' ? 'Isaac Daimiel Naranjo, Almir G. V. Bitencourt, Natsuko Onishi, Eun Sook Ko, Doris Leithner, Daly Avendano, Sarah Eskreis-Winkler, Mary Hughes, Danny F. Martinez, Katja Pinker, Krishna Juluru, Amin E. El-Rowmeim, Pierre Elnajjar, Elizabeth A. Morris, Hernan A. Makse, Lucas C. Parra, Elizabeth J. Sutton' : '17 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">17 more authors</span> </div> <div class="periodical"> <em>Radiology: Artificial Intelligence</em>, Jan 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://pubs.rsna.org/doi/full/10.1148/ryai.200231" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/andypotatohy/biomedical_segmenter" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=6dHFIA8AAAAJ&amp;citation_for_view=6dHFIA8AAAAJ:PR6Y55bgFSsC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-69-4285F4?logo=googlescholar&amp;labelColor=beige" alt="69 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Purpose To develop a deep network architecture that would achieve fully automated radiologist-level segmentation of cancers at breast MRI. Materials and Methods In this retrospective study, 38 229 examinations (composed of 64 063 individual breast scans from 14 475 patients) were performed in female patients (age range, 12–94 years; mean age, 52 years ± 10 [standard deviation]) who presented between 2002 and 2014 at a single clinical site. A total of 2555 breast cancers were selected that had been segmented on two-dimensional (2D) images by radiologists, as well as 60 108 benign breasts that served as examples of noncancerous tissue; all these were used for model training. For testing, an additional 250 breast cancers were segmented independently on 2D images by four radiologists. Authors selected among several three-dimensional (3D) deep convolutional neural network architectures, input modalities, and harmonization methods. The outcome measure was the Dice score for 2D segmentation, which was compared between the network and radiologists by using the Wilcoxon signed rank test and the two one-sided test procedure. Results The highest-performing network on the training set was a 3D U-Net with dynamic contrast-enhanced MRI as input and with intensity normalized for each examination. In the test set, the median Dice score of this network was 0.77 (interquartile range, 0.26). The performance of the network was equivalent to that of the radiologists (two one-sided test procedures with radiologist performance of 0.69–0.84 as equivalence bounds, P \textless .001 for both; n = 250). Conclusion When trained on a sufficiently large dataset, the developed 3D U-Net performed as well as fellowship-trained radiologists in detailed 2D segmentation of breast cancers at routine clinical MRI.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/roast2019.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="roast2019.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="huang_realistic_2019" class="col-sm-8"> <div class="title">Realistic volumetric-approach to simulate transcranial electric stimulation—ROAST—a fully automated open-source pipeline</div> <div class="author"> <em>Yu Huang</em>, Abhishek Datta, Marom Bikson, and Lucas C. Parra </div> <div class="periodical"> <em>Journal of Neural Engineering</em>, Jul 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://iopscience.iop.org/article/10.1088/1741-2552/ab208d/meta" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=-Yr53o4Sw2Y" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/andypotatohy/roast" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://neuromodec.org/nyc-neuromodulation-online-2020/Neuromod2020_ROASTtutorial.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a> <a href="https://www.parralab.org/roast/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=6dHFIA8AAAAJ&amp;citation_for_view=6dHFIA8AAAAJ:l7t_Zn2s7bgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-433-4285F4?logo=googlescholar&amp;labelColor=beige" alt="433 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Objective. Research in the area of transcranial electrical stimulation (TES) often relies on computational models of current flow in the brain. Models are built based on magnetic resonance images (MRI) of the human head to capture detailed individual anatomy. To simulate current flow on an individual, the subject’s MRI is segmented, virtual electrodes are placed on this anatomical model, the volume is tessellated into a mesh, and a finite element model (FEM) is solved numerically to estimate the current flow. Various software tools are available for each of these steps, as well as processing pipelines that connect these tools for automated or semi-automated processing. The goal of the present tool—realistic volumetric-approach to simulate transcranial electric simulation (ROAST)—is to provide an end-to-end pipeline that can automatically process individual heads with realistic volumetric anatomy leveraging open-source software and custom scripts to improve segmentation and execute electrode placement. Approach. ROAST combines the segmentation algorithm of SPM12, a Matlab script for touch-up and automatic electrode placement, the finite element mesher iso2mesh and the solver getDP. We compared its performance with commercial FEM software, and SimNIBS, a well-established open-source modeling pipeline. Main results. The electric fields estimated with ROAST differ little from the results obtained with commercial meshing and FEM solving software. We also do not find large differences between the various automated segmentation methods used by ROAST and SimNIBS. We do find bigger differences when volumetric segmentation are converted into surfaces in SimNIBS. However, evaluation on intracranial recordings from human subjects suggests that ROAST and SimNIBS are not significantly different in predicting field distribution, provided that users have detailed knowledge of SimNIBS. Significance. We hope that the detailed comparisons presented here of various choices in this modeling pipeline can provide guidance for future tool development. We released ROAST as an open-source, easy-to-install and fully-automated pipeline for individualized TES modeling.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/huangElife2017.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="huangElife2017.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="huang_measurements_2017" class="col-sm-8"> <div class="title">Measurements and models of electric fields in the in vivo human brain during transcranial electric stimulation</div> <div class="author"> <em>Yu Huang<sup>*</sup></em>, Anli A. Liu<sup>*</sup>, Belen Lafon, Daniel Friedman, Michael Dayan, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Xiuyuan Wang, Marom Bikson, Werner K. Doyle, Orrin Devinsky, Lucas C. Parra' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '1'); ">5 more authors</span> </div> <div class="periodical"> <em>eLife</em>, Feb 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://elifesciences.org/articles/18834" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=jg9HVpaROoI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://elifesciences.org/articles/25812" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://crcns.org/data-sets/methods/tes-1/about-tes-1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=6dHFIA8AAAAJ&amp;citation_for_view=6dHFIA8AAAAJ:XiSMed-E-HIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-640-4285F4?logo=googlescholar&amp;labelColor=beige" alt="640 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Transcranial electric stimulation aims to stimulate the brain by applying weak electrical currents at the scalp. However, the magnitude and spatial distribution of electric fields in the human brain are unknown. We measured electric potentials intracranially in ten epilepsy patients and estimate electric fields across the entire brain by leveraging calibrated current-flow models. When stimulating at 2 mA, cortical electric fields reach 0.4 V/m, the lower limit of effectiveness in animal studies. When individual whole-head anatomy is considered, the predicted electric field magnitudes correlate with the recorded values in cortical (r=0.89) and depth (r=0.84) electrodes. Accurate models require adjustment of tissue conductivity values reported in the literature, but accuracy is not improved when incorporating white matter anisotropy or different skull compartments. This is the first study to validate and calibrate current-flow models with in vivo intracranial recordings in humans, providing a solid foundation to target stimulation and interpret clinical trials.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/nyhead.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="nyhead.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="huang_new_2016" class="col-sm-8"> <div class="title">The New York Head—A precise standardized volume conductor model for EEG source localization and tES targeting</div> <div class="author"> <em>Yu Huang</em>, Lucas C. Parra, and Stefan Haufe </div> <div class="periodical"> <em>NeuroImage</em>, Oct 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S1053811915011325" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.parralab.org/nyhead/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=6dHFIA8AAAAJ&amp;citation_for_view=6dHFIA8AAAAJ:UxriW0iASnsC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-305-4285F4?logo=googlescholar&amp;labelColor=beige" alt="305 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>In source localization of electroencephalograpic (EEG) signals, as well as in targeted transcranial electric current stimulation (tES), a volume conductor model is required to describe the flow of electric currents in the head. Boundary element models (BEM) can be readily computed to represent major tissue compartments, but cannot encode detailed anatomical information within compartments. Finite element models (FEM) can capture more tissue types and intricate anatomical structures, but with the higher precision also comes the need for semi-automated segmentation, and a higher computational cost. In either case, adjusting to the individual human anatomy requires costly magnetic resonance imaging (MRI), and thus head modeling is often based on the anatomy of an ‘arbitrary’ individual (e.g. Colin27). Additionally, existing reference models for the human head often do not include the cerebro-spinal fluid (CSF), and their field of view excludes portions of the head and neck—two factors that demonstrably affect current-flow patterns. Here we present a highly detailed FEM, which we call ICBM-NY, or "New York Head". It is based on the ICBM152 anatomical template (a non-linear average of the MRI of 152 adult human brains) defined in MNI coordinates, for which we extended the field of view to the neck and performed a detailed segmentation of six tissue types (scalp, skull, CSF, gray matter, white matter, air cavities) at 0.5 mm 3 resolution. The model was solved for 231 electrode locations. To evaluate its performance, additional FEMs and BEMs were constructed for four individual subjects. Each of the four individual FEMs (regarded as the ‘ground truth’) is compared to its BEM counterpart, the ICBM-NY, a BEM of the ICBM anatomy, an ‘individualized’ BEM of the ICBM anatomy warped to the individual head surface, and FEMs of the other individuals. Performance is measured in terms of EEG source localization and tES targeting errors. Results show that the ICBM-NY outperforms FEMs of mismatched individual anatomies as well as the BEM of the ICBM anatomy according to both criteria. We therefore propose the New York Head as a new standard head model to be used in future EEG and tES studies whenever an individual MRI is not available. We release all model data online at neuralengr.com/nyhead/ to facilitate broad adoption.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:andypotatohy@gmail.com" title="Email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=6dHFIA8AAAAJ" title="Scholar userid" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/andypotatohy" title="Github username" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/yu-andy-huang-ph-d-92861533" title="Linkedin username" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/andypotato" title="X username" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> <a href="https://bsky.app/profile/andypotato.bsky.social" title="Bluesky url" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-bluesky"></i></a> <a href="https://www.researchgate.net/profile/Yu-Huang-60/" title="Research gate profile" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://orcid.org/0000-0003-4178-0739" title="Orcid id" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://publons.com/a/AFJ-8096-2022/" title="Publons id" rel="external nofollow noopener" target="_blank"><i class="ai ai-publons"></i></a> </div> <div class="contact-note">Please feel free to email me. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Yu (Andy) Huang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: February 24, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>